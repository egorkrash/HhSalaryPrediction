{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from DatasetCreation import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "data,y = make_dataset('data_with_important_features.csv', True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def topN(d,N):\n",
    "    c = Counter()\n",
    "    def cou(x):\n",
    "        c[x]= c[x]+1\n",
    "    d.map(cou)\n",
    "    \n",
    "    def setWord(x):\n",
    "        if c[x] > N:\n",
    "            return x\n",
    "        else:\n",
    "            return 'other'\n",
    "    return d.map(setWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбираем самых значимых"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['employer:name'] = topN(data['employer:name'] , 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area:name'] = topN(data['area:name'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name_0']=data['name'].iloc[0].split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: Учитывать Skill2Vec\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# категориальные колонки \n",
    "all_categorial = ['employer:name', 'name_0','specializations:0:profarea_name','specializations:1:profarea_name','specializations:2:profarea_name','specializations:3:profarea_name','specializations:4:profarea_name','specializations:5:profarea_name', 'area:name','employment:name','experience:name','schedule:name','billing_type:name']\n",
    "\n",
    "\n",
    "data_categ = data[all_categorial]\n",
    "\n",
    "# {имя колонки : LabelEncoder..}\n",
    "encNames = {}\n",
    "\n",
    "# учим LabelEncoder'ы и сохраняем их в словарь, а затем применяем энкодеры к all_data\n",
    "for name in all_categorial:\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(data_categ[name]) \n",
    "    encNames[name] = enc\n",
    "    data_categ[name] = enc.transform(data_categ[name])\n",
    "\n",
    "# заполняем null\n",
    "data_categ.fillna(-1, inplace=True)\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=data_categ.columns.isin(all_categorial),sparse=True)\n",
    "\n",
    "data_categ = ohe.fit_transform(data_categ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161544, 226)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_categ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Достанем данные о предсказании прошлых нейронок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pred = pkl.load(open('Ponyland/X_train_pred.pkl','rb'))\n",
    "test_pred = pkl.load(open('Ponyland/X_test_pred.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "#Зарпллаты в логарифмизированном виде распределены практически нормально\n",
    "Y = np.log(y+1)\n",
    "\n",
    "X_train, X_val, y_train,y_val, yy_train, yy_val = train_test_split(data_categ.toarray(), Y,y,random_state=42,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train,train_pred))\n",
    "X_val = np.hstack((X_val, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "params = { \n",
    "    'seed': 0, \n",
    "    'colsample_bytree': 0.8, \n",
    "    'silent': 1, \n",
    "    'subsample': 0.7, \n",
    "    'learning_rate': 0.03, \n",
    "    'objective': 'reg:linear', \n",
    "    'max_depth': 15, \n",
    "    'min_child_weight': 10, \n",
    "    'booster': 'gbtree'\n",
    "} \n",
    "\n",
    "#search_X1, search_X2, search_y1, search_y2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train) \n",
    "\n",
    "#watchlist = [(d_train, 'train'), (d_valid, 'eval')] \n",
    "\n",
    "\n",
    "\n",
    "clf = xgb.train(params, \n",
    "d_train, \n",
    "250)\n",
    "\n",
    "\n",
    "#reg = XGBClassifier(nthread=15,n_estimators = 200, max_depth=20,mea,learning_rate=0.05)\n",
    "#reg = RandomForestRegressor(n_estimators=50)\n",
    "#reg.fit(X_train,yy_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_test = xgb.DMatrix(X_val, label=y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20773.2845639 MSE, в рублях\n",
      "0.201785861744 Средняя относительная ошибка\n"
     ]
    }
   ],
   "source": [
    "predictions = np.exp(clf.predict(d_test))-1\n",
    "\n",
    "print np.sqrt(mean_squared_error(predictions,np.exp(y_val)-1)), 'MSE, в рублях'\n",
    "print (np.abs(predictions - np.exp(y_val)+1)/((np.exp(y_val)-1))).mean(),'Средняя относительная ошибка'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
